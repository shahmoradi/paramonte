<!-- HTML header for doxygen 1.9.6-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="$langISO">
<head>
    <meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=11"/>
    <meta name="generator" content="Doxygen 1.9.3"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <title>ParaMonte Fortran 2.0.0: pm_clusKmeans Module Reference</title>
    <link href="tabs.css" rel="stylesheet" type="text/css"/>
    <script type="text/javascript" src="jquery.js"></script>
    <script type="text/javascript" src="dynsections.js"></script>
    <link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
    <link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  extensions: ["tex2jax.js"],
  jax: ["input/TeX","output/HTML-CSS"],
   TeX: { Macros: {
    up: ["{{\\mathrm{#1}}}",1],
    bs: ["{{\\boldsymbol{#1}}}",1],
    ms: ["{\\texttt{#1}}",1],
    diff: "{{\\mathrm{d}}}",
    bu: ["{{\\boldsymbol{\\mathrm{#1}}}}",1],
    sline: "{\\rule{\\textwidth}{1pt}}",
    sphere: "{{\\mathcal{S}}}",
    ell: "{{\\mathcal{E}}}",
    ndim: "{{\\ms{ndim}}}",
    gramian: "{{\\mathcal{G}}}",
    mat: ["{{\\boldsymbol{\\mathrm {#1}}}}",1],
    unit: ["{{\\boldsymbol{\\widehat{\\mathrm{#1}}}}}",1],
    ebreak: "{{E_\\ms{b}}}",
    xbreak: "{{x_\\ms{b}}}",
    efold: "{{E_\\ms{f}}}",
    epeak: "{{E_\\ms{p}}}",
    phot: "{{\\ms{ph}}}",
    ergs: "{{\\ms{ergs}}}",
    sergs: "{{S_{\\ms{ergs}}}}",
    sphot: "{{S_{\\phot}}}",
    kev: "{{\\ms{keV}}}",
    det: "{{\\ms{det}}}",
    var: "{{\\ms{var}}}",
    binom: ["{\\Bigl(\\begin{array}{@{}c@{}}#1\\\\#2\\end{array}\\Bigr)}",2],
  } }
});
</script>
<script type="text/javascript" async="async" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js"></script>
    <link href="doxygen.css" rel="stylesheet" type="text/css" />
    <link href="html_extra_stylesheet.css" rel="stylesheet" type="text/css"/>
</head>
<body>
    <div id="top"><!-- do not remove this div, it is closed by doxygen! -->
        <div id="titlearea">
            <table cellspacing="0" cellpadding="0" stype="z-index:-1;">
                <tbody>
                    <tr id="projectrow">
                        <td id="projectlogo">
                            <a href="https://www.cdslab.org/paramonte/" target="_blank"><img alt="Logo" src="logo.png"/></a>
                        </td>
                        <td id="projectalign">
                            <div id="projectname">
                                ParaMonte Fortran 2.0.0
                            </div>
                            <div id="projectbrief">
                                Parallel Monte Carlo and Machine Learning Library<br><a href="../latest/index.html" target="_blank">See the latest version documentation.</a>
                            </div>
                        </td>
                        <!--
                        <td id="projectalign" style="padding-left: 0.5em;">
                            <a href="https://github.com/cdslaborg/paramonte/releases" target="_blank">
                                <div id="projectname">
                                    <span id="projectbrief">Parallel Monte Carlo &#38;</span>
                                    <span id="projectbrief">Machine Learning Library</span>
                                    <span id="projectbrief">Version </span>
                                </div>
                            </a>
                        </td>
                        <td style="padding-left: 0.5em;">
                            <div id="projectbrief">Parallel Monte Carlo and Machine Learning Library</div>
                        </td>
                        -->
                    </tr>
                </tbody>
            </table>
        </div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.3 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search",'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('namespacepm__clusKmeans.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="summary">
<a href="#nested-classes">Data Types</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle"><div class="title">pm_clusKmeans Module Reference</div></div>
</div><!--header-->
<div class="contents">

<p>This module contains procedures and routines for the computing the Kmeans clustering of a given set of data.  
<a href="namespacepm__clusKmeans.html#details">More...</a></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="nested-classes" name="nested-classes"></a>
Data Types</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">interface &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="interfacepm__clusKmeans_1_1setCenter.html">setCenter</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compute and return the centers of the clusters corresponding to the input <code>sample</code>, cluster <code>membership</code> IDs, and <code>sample</code> distances-squared from their corresponding cluster centers.<br  />
  <a href="interfacepm__clusKmeans_1_1setCenter.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">interface &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="interfacepm__clusKmeans_1_1setKmeans.html">setKmeans</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compute and return an iteratively-refined set of cluster centers given the input <code>sample</code> using the k-means approach.<br  />
  <a href="interfacepm__clusKmeans_1_1setKmeans.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">interface &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="interfacepm__clusKmeans_1_1setKmeansPP.html">setKmeansPP</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compute and return an asymptotically optimal set of cluster centers for the input <code>sample</code>, cluster <code>membership</code> IDs, and <code>sample</code> distances-squared from their corresponding cluster centers.<br  />
  <a href="interfacepm__clusKmeans_1_1setKmeansPP.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">interface &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="interfacepm__clusKmeans_1_1setMember.html">setMember</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compute and return the memberships and minimum distances of a set of input points with respect to the an input set of cluster centers.<br  />
  <a href="interfacepm__clusKmeans_1_1setMember.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="var-members" name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:a00fa2c4565da16bef172aeecb8f10ef5"><td class="memItemLeft" align="right" valign="top">character(*, SK), parameter&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacepm__clusKmeans.html#a00fa2c4565da16bef172aeecb8f10ef5">MODULE_NAME</a> = &quot;@pm_clusKmeans&quot;</td></tr>
<tr class="separator:a00fa2c4565da16bef172aeecb8f10ef5"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p >This module contains procedures and routines for the computing the Kmeans clustering of a given set of data. </p>
<p >The <b>k-means</b> clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into \(k\) clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster.<br  />
 This results in a partitioning of the data space into Voronoi cells.<br  />
 The k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem:<br  />
 the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances.<br  />
 For instance, better Euclidean solutions can be found using <b>k-medians</b> and <b>k-medoids</b>.<br  />
 The problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum.<br  />
 These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian mixture modeling.<br  />
 They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the Gaussian mixture model allows clusters to have different shapes.<br  />
</p>
<h1><a class="anchor" id="autotoc_md3"></a>
Kmeans Algorithm</h1>
<p >Given a set of observations \((x_1, x_2, \ldots, x_n)\), where each observation is a \(d\)-dimensional real vector, the k-means clustering aims to partition the \(n\) observations into \(k\) ( \(\leq n\)) sets \(S = \{S_1, S_2, \ldots, S_k\}\) so as to minimize the within-cluster sum of squares (WCSS) (i.e. variance).<br  />
 Formally, the objective is to find:<br  />
  </p><p class="formulaDsp">
\begin{equation}
      \underset{\mathbf{S}}{\up{arg\,min}}
      \sum_{i=1}^{k} \sum_{\mathbf{x} \in S_{i}} \left\|\mathbf{x} -{\boldsymbol{\mu}}_{i}\right\|^{2}
      = {\underset{\mathbf{S}}{\up{arg\,min}}}\sum_{i=1}^{k}|S_{i}|\up{Var}S_{i} ~,
  \end{equation}
</p>
<p> where \(\mu_i\) is the mean (also called <b>centroid</b>) of points in \(S_{i}\), i.e.  </p><p class="formulaDsp">
\begin{equation}
  {\boldsymbol {\mu_{i}}} = {\frac{1}{|S_{i}|}} \sum_{\mathbf{x} \in S_{i}} \mathbf{x} ~,
  \end{equation}
</p>
<p> where \(|S_{i}|\) is the size of \(S_{i}\), and \(\|\cdot\|\) is the \(L^2\)-norm.<br  />
 This is equivalent to minimizing the pairwise squared deviations of points in the same cluster:<br  />
  </p><p class="formulaDsp">
\begin{equation}
      \underset{\mathbf{S}}{\up{arg\,min}} \sum_{i=1}^{k}\,{\frac {1}{|S_{i}|}}\,\sum_{\mathbf{x}, \mathbf{y} \in S_{i}}\left\|\mathbf{x} - \mathbf{y} \right\|^{2} ~,
  \end{equation}
</p>
<p> The equivalence can be deduced from identity  </p><p class="formulaDsp">
\begin{equation}
      |S_{i}|\sum_{\mathbf{x} \in S_{i}}\left\|\mathbf{x} -{\boldsymbol{\mu}}_{i}\right\|^{2} =
      {\frac{1}{2}}\sum _{\mathbf {x} ,\mathbf {y} \in S_{i}}\left\|\mathbf {x} -\mathbf {y} \right\|^{2} ~.
  \end{equation}
</p>
<p> Since the total variance is constant, this is equivalent to maximizing the sum of squared deviations between points in different clusters.<br  />
 This deterministic relationship is also related to the law of total variance in probability theory.<br  />
</p>
<h2><a class="anchor" id="autotoc_md4"></a>
Kmeans Performance improvements</h2>
<p >The k-means++ seeding method yields considerable improvement in the final error of k-means algorithm.<br  />
 For more information, see the documentation of <a class="el" href="interfacepm__clusKmeans_1_1setKmeansPP.html">setKmeansPP</a>.<br  />
</p>
<p >In data mining, the <b>k-means++</b> is an algorithm for choosing the initial values (or <b>seeds</b>) for the <a class="el" href="interfacepm__clusKmeans_1_1setKmeans.html">k-means clustering algorithm</a>.<br  />
 It was proposed in 2007 by David Arthur and Sergei Vassilvitskii, as an approximation algorithm for the NP-hard k-means problem.<br  />
 It offers a way of avoiding the sometimes poor clustering found by the standard k-means algorithm.<br  />
</p>
<h2><a class="anchor" id="autotoc_md5"></a>
Kmeans++ Intuition</h2>
<p >The intuition behind k-means++ is that spreading out the \(k\) initial cluster centers is a good thing:<br  />
 The first cluster center is chosen uniformly at random from the data points that are being clustered, after which each subsequent cluster center is chosen from the remaining data points with probability proportional to its squared distance from the closest existing cluster center to the point.<br  />
</p>
<h2><a class="anchor" id="autotoc_md6"></a>
Kmeans++ Algorithm</h2>
<p >The exact algorithm is as follows:<br  />
 </p><ol>
<li>
Choose one center uniformly at random among the data points.<br  />
 </li>
<li>
For each data point \(x\) not chosen yet, compute \(D(x)\), the distance between \(x\) and the nearest center that has already been chosen.<br  />
 </li>
<li>
Choose one new data point at random as a new center, using a weighted probability distribution where a point \(x\) is chosen with probability proportional to \(D(x)^2\).<br  />
 </li>
<li>
Repeat Steps 2 and 3 until \(k\) centers have been chosen.<br  />
 </li>
<li>
Now that the initial centers have been chosen, proceed using standard k-means clustering.<br  />
 </li>
</ol>
<h2><a class="anchor" id="autotoc_md7"></a>
Kmeans++ Performance improvements</h2>
<p >The k-means++ seeding method yields considerable improvement in the final error of k-means algorithm.<br  />
 Although the initial selection in the algorithm takes extra time, the k-means part itself converges very quickly after this seeding and thus the algorithm actually lowers the computation time.<br  />
 Based on the original paper, the method yields typically 2-fold improvements in speed, and for certain datasets, close to 1000-fold improvements in error.<br  />
 In these simulations the new method almost always performed at least as well as vanilla k-means in both speed and error.<br  />
</p>
<dl class="test"><dt><b><a class="el" href="test.html#_test000144">Test:</a></b></dt><dd>test_pm_clusKmeans<br  />
</dd></dl>
<p ><br  />
<a class="anchor" id="final"></a><b>Final Remarks</b> <a href="#final">â›“</a> </p><hr  />
<p> If you believe this algorithm or its documentation can be improved, <b>we appreciate your contribution and help</b> <a href="https://github.com/cdslaborg/paramonte/tree/main/src/fortran/main/pm_clusKmeans.F90#L106 " target="_blank"><b>to edit this page's documentation and source file on GitHub</b></a>.<br  />
 For details on the naming abbreviations, see <a class="el" href="index.html#ParaMonteLangAbbreviationGuidlines">this page</a>.<br  />
 For details on the naming conventions, see <a class="el" href="index.html#ParaMonteLangNamingConventions">this page</a>.<br  />
 This software is distributed under the <a href="https://github.com/cdslaborg/paramonte#license" target="_blank">MIT license</a> <b>with additional terms outlined below.</b><br  />
</p><ol>
<li>
If you use any parts or concepts from this library to any extent, <b>please acknowledge the usage by citing the relevant</b> <a href="https://www.cdslab.org/paramonte/generic/latest/overview/preface/#how-to-acknowledge-the-use-of-the-paramonte-library-in-your-work" target="_blank"><b>publications of the ParaMonte library</b></a>.<br  />
</li>
<li>
If you regenerate any parts/ideas from this library in a programming environment other than those currently supported by this ParaMonte library (i.e., other than C, C++, Fortran, MATLAB, Python, R), <b>please also ask the end users to</b> <a href="https://www.cdslab.org/paramonte/generic/latest/overview/preface/#how-to-acknowledge-the-use-of-the-paramonte-library-in-your-work" target="_blank"><b>cite this original ParaMonte library</b></a>.<br  />
</li>
</ol>
<p>This software is available to the public under a highly permissive license.<br  />
Help us justify its continued development and maintenance by acknowledging its benefit to society, distributing it, and contributing to it.<br  />
 </p><dl class="section copyright"><dt>Copyright</dt><dd><a href="https://www.cdslab.org" target="_blank">Computational Data Science Lab</a></dd></dl>
<dl class="authors"><dt><b><a class="el" href="authors.html#_authors000214">Author:</a></b></dt><dd><a href="https://www.github.com/shahmoradi" target="_blank">Amir Shahmoradi</a>, April 03, 2017, 2:16 PM, Institute for Computational Engineering and Sciences (ICES), University of Texas at Austin </dd></dl>
</div><h2 class="groupheader">Variable Documentation</h2>
<a id="a00fa2c4565da16bef172aeecb8f10ef5" name="a00fa2c4565da16bef172aeecb8f10ef5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a00fa2c4565da16bef172aeecb8f10ef5">&#9670;&nbsp;</a></span>MODULE_NAME</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">character(*, SK), parameter pm_clusKmeans::MODULE_NAME = &quot;@pm_clusKmeans&quot;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="pm__clusKmeans_8F90_source.html#l00120">120</a> of file <a class="el" href="pm__clusKmeans_8F90_source.html">pm_clusKmeans.F90</a>.</p>

</div>
</div>
</div><!-- contents -->
</div><!-- doc-content -->
    <!-- start footer part -->
    <div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
      <ul>
        <li class="navelem"><a class="el" href="namespacepm__clusKmeans.html">pm_clusKmeans</a></li>
        <li class="footer">Generated on Thu Nov 7 2024 13:50:00 for ParaMonte Fortran 2.0.0 by <a href="http://www.doxygen.org/index.html" target="_blank"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.3 </li>
      </ul>
    </div>
    <div style="color: #ffffff; float: none; width: 100%;">
        <span style="font-size:0.8em">
            <script type="text/javascript">
                var sc_project=12178963; 
                var sc_invisible=1; 
                var sc_security="e0dfe0f9"; 
                var sc_text=3; 
                var scJsHost = "https://";
                document.write("<sc"+"ript type='text/javascript' src='" +
                scJsHost+
                "statcounter.com/counter/counter.js'></"+"script>");
            </script>
        </span>
    </div>
</body>
</html>
