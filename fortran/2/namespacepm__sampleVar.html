<!-- HTML header for doxygen 1.9.6-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="$langISO">
<head>
    <meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=11"/>
    <meta name="generator" content="Doxygen 1.9.3"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <title>ParaMonte Fortran 2.0.0: pm_sampleVar Module Reference</title>
    <link href="tabs.css" rel="stylesheet" type="text/css"/>
    <script type="text/javascript" src="jquery.js"></script>
    <script type="text/javascript" src="dynsections.js"></script>
    <link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
    <link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  extensions: ["tex2jax.js"],
  jax: ["input/TeX","output/HTML-CSS"],
   TeX: { Macros: {
    up: ["{{\\mathrm{#1}}}",1],
    bs: ["{{\\boldsymbol{#1}}}",1],
    ms: ["{\\texttt{#1}}",1],
    diff: "{{\\mathrm{d}}}",
    bu: ["{{\\boldsymbol{\\mathrm{#1}}}}",1],
    sline: "{\\rule{\\textwidth}{1pt}}",
    sphere: "{{\\mathcal{S}}}",
    ell: "{{\\mathcal{E}}}",
    ndim: "{{\\ms{ndim}}}",
    gramian: "{{\\mathcal{G}}}",
    mat: ["{{\\boldsymbol{\\mathrm {#1}}}}",1],
    unit: ["{{\\boldsymbol{\\widehat{\\mathrm{#1}}}}}",1],
    ebreak: "{{E_\\ms{b}}}",
    xbreak: "{{x_\\ms{b}}}",
    efold: "{{E_\\ms{f}}}",
    epeak: "{{E_\\ms{p}}}",
    phot: "{{\\ms{ph}}}",
    ergs: "{{\\ms{ergs}}}",
    sergs: "{{S_{\\ms{ergs}}}}",
    sphot: "{{S_{\\phot}}}",
    kev: "{{\\ms{keV}}}",
    det: "{{\\ms{det}}}",
    var: "{{\\ms{var}}}",
  } }
});
</script>
<script type="text/javascript" async="async" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js"></script>
    <link href="doxygen.css" rel="stylesheet" type="text/css" />
    <link href="html_extra_stylesheet.css" rel="stylesheet" type="text/css"/>
</head>
<body>
    <div id="top"><!-- do not remove this div, it is closed by doxygen! -->
        <div id="titlearea">
            <table cellspacing="0" cellpadding="0" stype="z-index:-1;">
                <tbody>
                    <tr id="projectrow">
                        <td id="projectlogo">
                            <a href="https://www.cdslab.org/paramonte/" target="_blank"><img alt="Logo" src="logo.png"/></a>
                        </td>
                        <td id="projectalign">
                            <div id="projectname">
                                ParaMonte Fortran 2.0.0
                            </div>
                            <div id="projectbrief">
                                Parallel Monte Carlo and Machine Learning Library<br><a href="../latest/index.html" target="_blank">See the latest version documentation.</a>
                            </div>
                        </td>
                        <!--
                        <td id="projectalign" style="padding-left: 0.5em;">
                            <a href="https://github.com/cdslaborg/paramonte/releases" target="_blank">
                                <div id="projectname">
                                    <span id="projectbrief">Parallel Monte Carlo &#38;</span>
                                    <span id="projectbrief">Machine Learning Library</span>
                                    <span id="projectbrief">Version </span>
                                </div>
                            </a>
                        </td>
                        <td style="padding-left: 0.5em;">
                            <div id="projectbrief">Parallel Monte Carlo and Machine Learning Library</div>
                        </td>
                        -->
                    </tr>
                </tbody>
            </table>
        </div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.3 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search",'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('namespacepm__sampleVar.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="summary">
<a href="#nested-classes">Data Types</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle"><div class="title">pm_sampleVar Module Reference</div></div>
</div><!--header-->
<div class="contents">

<p>This module contains classes and procedures for computing the properties related to the covariance matrices of a random sample.<br  />
  
<a href="namespacepm__sampleVar.html#details">More...</a></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="nested-classes" name="nested-classes"></a>
Data Types</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">interface &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="interfacepm__sampleVar_1_1getVar.html">getVar</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Generate and return the variance of the input sample of type <code>complex</code> or <code>real</code> of shape <code>(nsam)</code> or <code>(ndim, nsam)</code> or <code>(nsam, ndim)</code> where <code>ndim</code> is the number of data dimensions (the number of data attributes) and <code>nsam</code> is the number of data points.<br  />
  <a href="interfacepm__sampleVar_1_1getVar.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">interface &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="interfacepm__sampleVar_1_1getVarCorrection.html">getVarCorrection</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Generate and return the bias correction factor for the computation of the variance of a (weighted) sample.<br  />
  <a href="interfacepm__sampleVar_1_1getVarCorrection.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">interface &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="interfacepm__sampleVar_1_1getVarMerged.html">getVarMerged</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Generate and return the (weighted) merged variance of a <code>complex</code> or <code>real</code> sample resulting from the merger of two separate (weighted) samples \(A\) and \(B\).<br  />
  <a href="interfacepm__sampleVar_1_1getVarMerged.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">interface &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="interfacepm__sampleVar_1_1setVar.html">setVar</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return the variance of an input (weighted) sample of type <code>complex</code> or <code>real</code> of shape <code>(nsam)</code> or <code>(ndim, nsam)</code> or <code>(nsam, ndim)</code> where <code>ndim</code> is the number of data dimensions (the number of data attributes) and <code>nsam</code> is the number of data points.<br  />
  <a href="interfacepm__sampleVar_1_1setVar.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">interface &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="interfacepm__sampleVar_1_1setVarMean.html">setVarMean</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return the (weighted) sample variance and mean of an input time series of <code>nsam</code> observations, or of an input <code>sample</code> of <code>nsam</code> observations with <code>ndim</code> attributes optionally weighted by the input <code>weight</code>, optionally also <code>sum(weight)</code>.<br  />
  <a href="interfacepm__sampleVar_1_1setVarMean.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">interface &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="interfacepm__sampleVar_1_1setVarMeanMerged.html">setVarMeanMerged</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return the (weighted) merged variance and mean of a <code>complex</code> or <code>real</code> sample resulting from the merger of two separate (weighted) samples \(A\) and \(B\).<br  />
  <a href="interfacepm__sampleVar_1_1setVarMeanMerged.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">interface &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="interfacepm__sampleVar_1_1setVarMerged.html">setVarMerged</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return the (weighted) merged variance of a <code>complex</code> or <code>real</code> sample resulting from the merger of two separate (weighted) samples \(A\) and \(B\).<br  />
  <a href="interfacepm__sampleVar_1_1setVarMerged.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="var-members" name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:ad9c411a07e915cb9810cf2091d79d67b"><td class="memItemLeft" align="right" valign="top">character(*, SK), parameter&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacepm__sampleVar.html#ad9c411a07e915cb9810cf2091d79d67b">MODULE_NAME</a> = &quot;@pm_sampleVar&quot;</td></tr>
<tr class="separator:ad9c411a07e915cb9810cf2091d79d67b"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p >This module contains classes and procedures for computing the properties related to the covariance matrices of a random sample.<br  />
 </p>
<h1><a class="anchor" id="autotoc_md119"></a>
Variance</h1>
<p >Variance is the squared deviation from the mean of a random variable.<br  />
 The variance is also often defined as the square of the standard deviation.<br  />
 Variance is a measure of dispersion, meaning it is a measure of how far a set of numbers is spread out from their average value.<br  />
 It is the second central moment of a distribution, and the covariance of the random variable with itself.<br  />
 It is frequently represented by \(\Sigma\), \(\sigma^2\), \(s^2\), \(\up{Var}(X)\), or \(\mathbb{V}(X)\).<br  />
</p>
<h2><a class="anchor" id="autotoc_md120"></a>
Variance as a measure of dispersion</h2>
<p >An advantage of variance as a measure of dispersion is that it is more amenable to algebraic manipulation than other measures of dispersion such as the expected absolute deviation.<br  />
 For example, the variance of a sum of uncorrelated random variables is equal to the sum of their variances.<br  />
 A disadvantage of the variance for practical applications is that, unlike the standard deviation, its units differ from the random variable, which is why the standard deviation is more commonly reported as a measure of dispersion once the calculation is finished.<br  />
</p>
<h2><a class="anchor" id="autotoc_md121"></a>
Population vs. sample variance</h2>
<p >There are two distinct concepts that are both called <em>variance</em>.<br  />
 One, as discussed above, is part of a theoretical probability distribution and is <b>defined by an equation</b>.<br  />
 The other variance is a <b>characteristic of a set of observations</b>.<br  />
 When variance is calculated from observations, those observations are typically measured from a real world system.<br  />
 If all possible observations of the system are present then the calculated variance is called the <b>population variance</b>.<br  />
 Normally, however, only a subset is available, and the variance calculated from this is called the <b>sample variance</b>.<br  />
 The variance calculated from a sample is considered an estimate of the full population variance.<br  />
 There are multiple ways to calculate an <b>estimate</b> of the population variance, as discussed in the section below.<br  />
 The two kinds of variance are closely related.<br  />
 To see how, consider that a theoretical probability distribution can be used as a generator of hypothetical observations.<br  />
 If an infinite number of observations are generated using a distribution, then the sample variance calculated from that infinite set will match the value calculated using the distribution's equation for variance.<br  />
</p>
<h2><a class="anchor" id="autotoc_md122"></a>
History</h2>
<p >The term <b>variance</b> was apparently first introduced by Ronald Fisher in his 1918 paper <em>The Correlation Between Relatives on the Supposition of Mendelian Inheritance</em>.<br  />
</p>
<h2><a class="anchor" id="autotoc_md123"></a>
Definition</h2>
<p >The variance of a random variable \(X\) is the expected value of the squared deviation from the mean of \(X\), \(\mu= \up{E}[X]\):<br  />
  </p><p class="formulaDsp">
\begin{equation}
      \up{Var}(X) = \up{E}\left[(X - \mu)^2\right] ~.
  \end{equation}
</p>
<p> This definition encompasses random variables that are generated by processes that are discrete, continuous, neither, or mixed.<br  />
 The variance can also be thought of as the covariance of a random variable with itself:<br  />
  </p><p class="formulaDsp">
\begin{equation}
      \up{Var}(X) = \up{Cov}(X, X) ~.
  \end{equation}
</p>
<p> The variance is also equivalent to the second <b>cumulant</b> of the probability distribution that generates \(X\).<br  />
 The variance is typically designated as \(\up{Var}(X)\), or sometimes as \(V(X)\) or \(\mathbb{V}(X)\), or symbolically as \(\sigma_X^2\) or simply \(\sigma^2\).<br  />
 The expression for the variance can be expanded as follows:<br  />
  </p><p class="formulaDsp">
\begin{equation}
      \begin{aligned}
          \up{Var}(X)
          &amp;= \up{E} \left[ (X - \up{E}[X])^2\right] \\
          &amp;= \up{E} \left[ X^2 - 2X\up{E}[X] + \up{E} [X]^{2} \right] \\
          &amp;= \up{E} \left[ X^2 \right] - 2\up{E}[X]\up{E}[X] + \up{E}[X]^{2} \\
          &amp;= \up{E} \left[ X^2 \right] - \up{E}[X]^{2}
      \end{aligned}
  \end{equation}
</p>
<p> In other words, the variance of \(X\) is equal to the mean of the square of \(X\) minus the square of the mean of \(X\).&lt;br&lt; However, this formulation is never used for computations using floating point arithmetic, because it suffers from <a href="https://en.wikipedia.org/wiki/Catastrophic_cancellation">catastrophic cancellation</a> if the two components of the equation are similar in magnitude.<br  />
</p>
<h2><a class="anchor" id="autotoc_md124"></a>
Population variance and sample variance</h2>
<p >Real-world observations such as the measurements of yesterday rain throughout the day typically cannot be complete sets of all possible observations that could be made.<br  />
 As such, the variance calculated from the finite set will in general not match the variance that would have been calculated from the full population of possible observations.<br  />
 This means that one estimates the mean and variance from a limited set of observations by using an estimator equation.<br  />
 The estimator is a function of the sample of \(n\) observations drawn without observational bias from the whole population of potential observations.<br  />
 The simplest estimators for population mean and population variance are simply the mean and variance of the sample, the sample mean and (uncorrected) sample variance.<br  />
 These are consistent estimators as they converge to the correct value as the number of samples increases), but can be improved.<br  />
 Estimating the population variance by taking the sample variance is close to optimal in general, but can be improved in two ways.<br  />
 The sample variance is computed as an average of squared deviations about the (sample) mean, by dividing by \(n\).<br  />
 However, using values other than \(n\) improves the estimator in various ways.<br  />
 Four common values for the denominator are \(n\), \(n − 1\), \(n + 1\), and \(n − 1.5\):<br  />
 </p><ol>
<li>
\(n\) is the simplest (population variance of the sample), </li>
<li>
\(n − 1\) eliminates bias, </li>
<li>
\(n + 1\) minimizes mean squared error for the normal distribution, </li>
<li>
\(n − 1.5\) mostly eliminates bias in unbiased estimation of standard deviation for the normal distribution.<br  />
 </li>
</ol>
<p>Firstly, if the true population mean is unknown, then the sample variance (which uses the sample mean in place of the true mean) is a biased estimator:<br  />
 It underestimates the variance by a factor of \((n − 1) / n\).<br  />
 Correcting by this factor (dividing by \(n − 1\) instead of \(n\)) is called the <a href="https://en.wikipedia.org/wiki/Bessel%27s_correction">Bessel correction</a>.<br  />
 The resulting estimator is unbiased, and is called the <b>corrected sample variance</b> or <b>unbiased sample variance</b>.<br  />
 For example, when \(n = 1\) the variance of a single observation about the sample mean (itself) is obviously zero regardless of the population variance.<br  />
 If the mean is determined in some other way than from the same samples used to estimate the variance then this bias does not arise and the variance can safely be estimated as that of the samples about the (independently known) mean.<br  />
</p>
<h2><a class="anchor" id="autotoc_md125"></a>
Biased sample variance</h2>
<p >In many practical situations, the true variance of a population is not known a priori and must be computed.<br  />
 When dealing with extremely large populations, it is not possible to count every object in the population, so the computation must be performed on a sample of the population.<br  />
 This is generally referred to as sample variance or empirical variance.<br  />
 Sample variance can also be applied to the estimation of the variance of a continuous distribution from a sample of that distribution.<br  />
 We take a sample with replacement of \(n\) values \(X_1, \ldots, X_n\) from the population and estimate the variance on the basis of this sample.<br  />
 Directly taking the variance of the sample data gives the average of the squared deviations:<br  />
  </p><p class="formulaDsp">
\begin{equation}
      \tilde{\sigma}_X^2 = \frac{1}{n} \sum_{i = 1}^{n} \left( X_i - \overline{X} \right)^2 = \left( \frac{1}{n} \sum_{i = 1}^{n} X_i^2 \right) - \overline{X}^2 = \frac{1}{n^2} \sum_{i, j ~:~ i &lt; j} \left( X_i - X_j \right)^2 ~.&lt;br&gt;
  \end{equation}
</p>
<p> Here, \(\overline{X}\) denotes the sample mean:<br  />
  </p><p class="formulaDsp">
\begin{equation}
      \overline{X} = \frac{1}{n} \sum_{i = 1}^n X_i ~.
  \end{equation}
</p>
<p> Since the \(X_i\) are selected randomly, both \(\overline{X}\) and \(\tilde{\sigma}_X^2\) are random variables.<br  />
 Their expected values can be evaluated by averaging over the ensemble of all possible samples \(X_i\) of size \(n\) from the population.<br  />
 For \(\tilde{\sigma}_X^2\) this gives:<br  />
  </p><p class="formulaDsp">
\begin{equation}
      \begin{aligned}
          \up{E}[\tilde{\sigma}_X^2]
          &amp;= \up{E} \left[ \frac{1}{n} \sum_{i = 1}^n \left( X_i - \frac{1}{n} \sum_{j = 1}^n X_{j} \right)^2 \right] \\
          &amp;= \frac{1}{n} \sum_{i = 1}^n \up{E} \left[ X_i^2 - \frac{2}{n} X_i \sum_{j = 1}^n X_j + \frac{1}{n^2} \sum_{j = 1}^n X_j \sum_{k = 1}^n X_k \right] \\
          &amp;= \frac{1}{n} \sum_{i = 1}^n \left( \frac{n - 2}{n} \up{E} \left[ X_i^2 \right] - \frac{2}{n} \sum_{j \neq i} \up{E} \left[ X_i X_j \right] + \frac{1}{n^2} \sum_{j = 1}^n \sum_{k \neq j}^n \up{E} \left[ X_j X_k \right] + \frac{1}{n^2} \sum_{j = 1}^n \up{E}\left[ X_j^2 \right] \right) \\
          &amp;= \frac{1}{n} \sum_{i = 1}^n \left[ \frac{n - 2}{n} \left( \sigma^2 + \mu^2 \right) - \frac{2}{n}(n - 1)\mu^2 + \frac{1}{n^2} n (n - 1) \mu^2 + \frac{1}{n} \left(\sigma^2 + \mu^2 \right)\right] \\
          &amp;= \frac{n - 1}{n} \sigma^2 ~.
      \end{aligned}
  \end{equation}
</p>
<p> Hence \(\tilde{\sigma}_X^2\) gives an estimate of the population variance that is biased by a factor of \(\frac{n - 1}{n}\).<br  />
 For this reason, \(\tilde{\sigma}_X^2\) is referred to as the <b>biased sample variance</b>.<br  />
 The bias-correction factor in this case is \(\xi = \frac{n}{n - 1}\).<br  />
</p>
<h2><a class="anchor" id="autotoc_md126"></a>
Unbiased sample variance</h2>
<p >Correcting for this bias yields the unbiased sample variance, denoted \(\sigma^2\):<br  />
  </p><p class="formulaDsp">
\begin{equation}
      \sigma^{2} = \frac{n}{n-1} \tilde{\sigma}_X^2 = \frac{n}{n - 1} \left[ \frac{1}{n} \sum_{i = 1}^n \left(X_i - \overline{X} \right)^2\right] = \frac{1}{n-1}\sum_{i = 1}^n\left(X_i - \overline{X} \right)^2 ~.
  \end{equation}
</p>
<p> Either estimator may be simply referred to as the sample variance when the version can be determined by context.<br  />
 The same proof is also applicable for samples taken from a continuous probability distribution.<br  />
 The use of the term \(n − 1\) is called <b>the Bessel correction</b>, and it is also used in sample covariance and the sample standard deviation (the square root of variance).<br  />
 The square root is a concave function and thus introduces negative bias (by the Jensen inequality), which depends on the distribution, and thus the corrected sample standard deviation is biased.<br  />
 The unbiased estimation of standard deviation is a technically involved problem, though for the normal distribution using the term \(n − 1.5\) yields an almost unbiased estimator.<br  />
</p>
<h2><a class="anchor" id="autotoc_md127"></a>
Biased weighted sample variance</h2>
<p >Given a <b>weighted sample</b> of \(n\) observations \(X_{1:n}\) with a <a class="el" href="namespacepm__sampleMean.html">weighted sample mean</a> \(\hat\mu_w\), the biased variance of the weighted sample is computed as,  </p><p class="formulaDsp">
\begin{equation}
      \hat\sigma_w^2 = \frac{1}{\sum_{i=1}^{n} w_i} \sum_{i = 1}^n w_i (X_i - \hat\mu_w)^2 ~,
  \end{equation}
</p>
<p> where <code>n = nsam</code> is the number of observations in the sample, \(w_i\) are the weights of individual data points, and \(\hat\mu_w\) is the <b>weighted mean</b> of the sample.<br  />
 When the sample size is small, the above equation yields a biased estimate of the variance.<br  />
</p>
<h2><a class="anchor" id="autotoc_md128"></a>
Unbiased weighted sample variance</h2>
<p >There is no unique generic equation for the <b>unbiased variance</b> of a <b>weighted sample</b>.<br  />
 However, depending on the types of the weights involved, a few popular definitions exist.<br  />
</p>
<ol>
<li>
The <b>unbiased variance</b> of a sample with <b>frequency</b>, <b>count</b>, or <b>repeat</b> weights can be computed via the following equation,  <p class="formulaDsp">
\begin{equation}
                  \hat\sigma_w^2
                  = \frac{\xi}{\sum_{i=1}^{n} w_i} \sum_{i=1}^{n} w_i (X_i - \hat\mu_w)^2
                  = \frac{1}{\left( \sum_{i=1}^{n} w_i \right) - 1} \sum_{i=1}^{n} w_i (X_i - \hat\mu_w)^2 ~,
              \end{equation}
</p>
 where the bias correction factor \(\xi\) is,  <p class="formulaDsp">
\begin{equation}
                  \xi = \frac{\sum_{i=1}^{n} w_i}{\left( \sum_{i=1}^{n} w_i \right) - 1} ~.
              \end{equation}
</p>
 <a class="el" href="structpm__sampleWeight_1_1fweight__type.html">Frequency weights</a> represent the number of duplications of each observation in the sample whose population variance is to be estimated.<br  />
 Therefore, the <b>frequency weights</b> are expected to be <b>integers</b> or <b>whole numbers</b>.<br  />
 </li>
<li>
The <b>unbiased variance</b> of a sample with <b>reliability weights</b>, also sometimes confusingly known as <b>probability weights</b> or <b>importance weights</b>, can be computed by the following equation,  <p class="formulaDsp">
\begin{equation}
                  \hat\sigma_w^2
                  = \frac{\xi}{\sum_{i=1}^{n}w_i} \sum_{i=1}^{n} w_i \left(X_i - \hat\mu_w\right)^2
                  = \frac{\sum_{i=1}^{n} w_i}{\left(\sum_{i=1}^{n}w_i\right)^2 - \left(\sum_{i=1}^{n}w_i^2\right)} \sum_{i=1}^{n} w_i \left(X_i - \hat\mu_w\right)^2
                  ~,
              \end{equation}
</p>
 where the bias correction factor \(\xi\) is,  <p class="formulaDsp">
\begin{equation}
                  \xi = \frac{\left(\sum_{i=1}^{n} w_i\right)^2}{\left(\sum_{i=1}^{n}w_i\right)^2 - \left(\sum_{i=1}^{n}w_i^2\right)} ~.
              \end{equation}
</p>
 <ol>
<li>
<a class="el" href="structpm__sampleWeight_1_1rweight__type.html">Reliability weights</a> weights, also known as <b>reliability weights</b> or <b>sampling weights</b> represent the probability of a case (or subject) being selected into the sample from a population.<br  />
 </li>
<li>
Application of the term <em>unbiased</em> to the above equation is controversial as some believe that bias cannot be correct without the knowledge of the sample size, which is lost in normalized weights.<br  />
 </li>
<li>
Reliability weights are frequently (but not necessarily) normalized, meaning that \(\sum^{i = 1}_{n} w_i = 1\).<br  />
 </li>
</ol>
</li>
</ol>
<h2><a class="anchor" id="autotoc_md129"></a>
Variance updating</h2>
<p >Consider a sample \(X_{1..n}\) of size \(n\) with weights \(w_{1..n}\).<br  />
 The weighted mean of this sample can be expressed as,  </p><p class="formulaDsp">
\begin{equation}
      \large
      \hat\mu_{1:n} = \frac{1}{w_{1:n}} \sum_{i = 1}^{n} w_i X_i ~,
  \end{equation}
</p>
<p> where \(w_{1:n} = \sum w_{1..n}\) and the weighted <b>biased</b> variance of the sample can be expressed as,  </p><p class="formulaDsp">
\begin{eqnarray}
      \large
      \tilde{\sigma}_{1:n}^2 = \frac{\hat\sigma_{1:n}^2}{\xi_{1:n}}
      &amp;=&amp; \frac{1}{w_{1:n}} \sum_{i = 1}^{n} w_i (X_i - \hat\mu_{1:n})^2 ~,
      &amp;=&amp; \frac{1}{w_{1:n}} \sum_{i = 1}^{n} w_i X_i^2 - \xi_{1:n} \hat\mu_{1:n}^2 ~,
  \end{eqnarray}
</p>
<p> where \(\xi_{1:n}\) is the appropriate bias-correction factor (which can be one).<br  />
 This implies,  </p><p class="formulaDsp">
\begin{equation}
      \large
      \sum_{i = 1}^{n} w_i X_i^2 = w_{1:n} \left(\frac{\hat\sigma_{1:n}^2}{\xi_{1:n}} + \hat\mu_{1:n}\right) ~.
  \end{equation}
</p>
<p> The above can be used to express the variance of the sample \(X_{1..n+m}\) resulting from the merger of two separate samples \(X_{1..n}\) and \(X_{n+1..n+m}\) as,  </p><p class="formulaDsp">
\begin{equation}
      \large
      \frac{\hat\sigma_{1:n+m}^2}{\xi_{1:n+m}} = \frac{1}{w_{1:n+m}}
      \left(
          w_{1:n} \left( \frac{\hat\sigma_{1:n}^2}{\xi_{1:n}} + \hat\mu_{1:n}^2 \right) +
          w_{n+1:n+m} \left(\frac{\hat\sigma_{n+1:n+m}^2}{\xi_{n+1:n+m}} + \hat\mu_{n+1:n+m}^2 \right) -
          w_{1:n+m} \hat\mu_{1:n+m}^2
      \right) ~.
  \end{equation}
</p>
<dl class="section note"><dt>Note</dt><dd>Note the effects of bias-correction in computing the variance become noticeable only for sample sample sizes (i.e., when <code>nsam</code> is small).<br  />
</dd>
<dd>
For a two or higher-dimensional <code>sample</code>, if the variance is to be computed for the entire <code>sample</code> (as opposed to computing it along a particular dimension), simply pass <code>reshape(sample, shape = size(sample))</code> to the appropriate <a class="el" href="interfacepm__sampleVar_1_1getVar.html">getVar</a> interface.<br  />
 Alternatively, a 1D pointer of the same size as the multidimensional sample can be passed to the procedure.<br  />
</dd>
<dd>
While it is tempting to extend this generic interface to <code>weight</code> arguments of type <code>integer</code> or <code>real</code> of various kinds, such extensions do not appear to add any tangible benefits beyond making the interface more flexible for the end user.<br  />
 But such extensions would certainly make the maintenance and future extensions of this interface difficult and complex.<br  />
 In the case of <code>integer</code> (frequency) weights, <ol>
<li>
The summation <code>sum(weight)</code> involved in the computation may lead to an integer-overflow if individual weights are too large.<br  />
 </li>
<li>
Avoiding overflow would then require coercing the weights to <code>real</code> before summation, which add an extra layer of unnecessary type coercion.<br  />
 </li>
<li>
Furthermore, according to the coercion rules of the Fortran standard, if an <code>integer</code> is multiplied with a <code>real</code>, the <code>integer</code> value must be first converted to <code>real</code> of the same kind as the real value, then multiplied.<br  />
 </li>
<li>
The type coercion to <code>real</code> will have to happen a second time when the weights are multiplied with the data values.<br  />
 </li>
<li>
Each integer-real type coercion costs about a <code>real</code> multiplication on modern hardware (See, e.g., <a href="https://stackoverflow.com/a/28668349/2088694">this thread</a>).<br  />
 </li>
</ol>
By contrast, <ol>
<li>
Real-valued weights, even if the weights are counts, do not require type coercion if <code>real</code> values in the computation are of the same kind as is here.<br  />
 </li>
<li>
The floating-point multiplication tends to be faster than integer multiplication on most modern architecture.<br  />
 </li>
<li>
However, real-valued weight summation is 4-8 times more expensive then <code>integer</code> addition, but less than <code>real</code> multiplication.<br  />
 </li>
</ol>
Considering all factors in the above, there does not seem to exist any performance benefits with providing dedicated interfaces for <code>weight</code> arguments of different type and kind.<br  />
 The following list compares the cost and latencies of some of the basic operations involving <code>integer</code> and <code>real</code> numbers.<br  />
 <ol>
<li>
Central Processing Unit (CPU): <ol>
<li>
Integer add: 1 cycle </li>
<li>
32-bit integer multiply: 10 cycles </li>
<li>
64-bit integer multiply: 20 cycles </li>
<li>
32-bit integer divide: 69 cycles </li>
<li>
64-bit integer divide: 133 cycles </li>
</ol>
</li>
<li>
On-chip Floating Point Unit (FPU): <ol>
<li>
Floating point add: 4 cycles </li>
<li>
Floating point multiply: 7 cycles </li>
<li>
Double precision multiply: 8 cycles </li>
<li>
Floating point divide: 23 cycles </li>
<li>
Double precision divide: 36 cycles </li>
</ol>
</li>
</ol>
</dd></dl>
<h1><a class="anchor" id="autotoc_md130"></a>
Generalization of variance</h1>
<h2><a class="anchor" id="autotoc_md131"></a>
Variance of complex variables</h2>
<p >If \(x\) is a scalar complex-valued random variable, with values in \(\mathbb{C}\), then its variance is \(\up{E} \left[(x-\mu )(x-\mu )^{*}\right]\), where \(x^{*}\) is the complex conjugate of \(x\).<br  />
 This variance is a real scalar.<br  />
</p>
<h2><a class="anchor" id="autotoc_md132"></a>
Matrix variance of vector-valued variables</h2>
<p >If \(X\) is a vector-valued random variable, with values in \(\mathbb{R}^{n}\), and thought of as a column vector, then a natural generalization of variance is \(\up{E}\left[(X-\mu)(X-\mu)^{\up{T}}\right]\), where \(\mu = \up{E}(X)\) and \(X^{\up{T}}\) is the transpose of \(X\), and so is a row vector.<br  />
 The result is a positive semi-definite square matrix, commonly referred to as the <b>variance-covariance matrix</b> (or simply as the <a class="el" href="namespacepm__sampleCov.html">covariance matrix</a>).<br  />
</p>
<h2><a class="anchor" id="autotoc_md133"></a>
Matrix variance of complex vector-valued variables</h2>
<p >If \(X\) is a vector- and complex-valued random variable, with values in \(\mathbb {C}^{n}\), then the covariance matrix is \(\up{E} \left[(X-\mu)(X-\mu)^{\dagger}\right]\), where \(X^{\dagger}\) is the conjugate transpose of \(X\).<br  />
 This matrix is also positive semi-definite and square.<br  />
</p>
<h2><a class="anchor" id="autotoc_md134"></a>
Extension of scalar variance to higher dimensions</h2>
<p >Another generalization of variance for vector-valued random variables \(X\), which results in a scalar value rather than in a matrix, is the generalized variance \(\det(C)\), the determinant of the covariance matrix.<br  />
 The generalized variance can be shown to be related to the multidimensional scatter of points around their mean.<br  />
</p>
<p >A different generalization is obtained by considering the variance of the Euclidean distance between the random variable and its mean.<br  />
 This results in \(\up{E} \left[(X-\mu)^{\up {T}}(X-\mu)\right] = \up{tr}(C)\), which is the <b>trace of the covariance matrix</b>.<br  />
</p>
<h2><a class="anchor" id="autotoc_md135"></a>
Summary</h2>
<ol>
<li>
Variance is a measure of dispersion in data.<br  />
 </li>
<li>
If the population mean is known a priori independent of the current sample based upon which the variance is to be computed, then the computed sample variance is an <b>unbiased estimate</b> of the true population variance.<br  />
 </li>
<li>
If the population mean is unknown a priori and must be computed via the same sample based upon which the variance is to be computed, then the computed sample variance is an <b>biased estimate</b> of the true population variance.<br  />
 </li>
<li>
This variance bias can be corrected by applying appropriate correction factors to the computed variances.<br  />
 </li>
<li>
The most famous such correction is called the <a href="https://en.wikipedia.org/wiki/Bessel%27s_correction">Bessel correction</a>.<br  />
 </li>
</ol>
<dl class="section see"><dt>See also</dt><dd>[<a class="el" href="namespacepm__sampling.html" title="This module contains procedures and generic interfaces for the ParaMonte library sampler routines.">pm_sampling</a>](<a class="el" href="namespacepm__sampling.html" title="This module contains procedures and generic interfaces for the ParaMonte library sampler routines.">pm_sampling</a>)<br  />
 <a class="el" href="namespacepm__sampleACT.html">pm_sampleACT</a><br  />
 <a class="el" href="namespacepm__sampleCCF.html">pm_sampleCCF</a><br  />
 <a class="el" href="namespacepm__sampleCor.html">pm_sampleCor</a><br  />
 <a class="el" href="namespacepm__sampleCov.html">pm_sampleCov</a><br  />
 pm_sampleConv<br  />
 <a class="el" href="namespacepm__sampleECDF.html">pm_sampleECDF</a><br  />
 <a class="el" href="namespacepm__sampleMean.html">pm_sampleMean</a><br  />
 <a class="el" href="namespacepm__sampleNorm.html">pm_sampleNorm</a><br  />
 <a class="el" href="namespacepm__sampleQuan.html">pm_sampleQuan</a><br  />
 <a class="el" href="namespacepm__sampleScale.html">pm_sampleScale</a><br  />
 <a class="el" href="namespacepm__sampleShift.html">pm_sampleShift</a><br  />
 <a class="el" href="namespacepm__sampleWeight.html">pm_sampleWeight</a><br  />
 <a class="el" href="namespacepm__sampleAffinity.html">pm_sampleAffinity</a><br  />
 <a class="el" href="namespacepm__sampleVar.html">pm_sampleVar</a><br  />
 Box and Tiao, 1973, <em>Bayesian Inference in Statistical Analysis</em>, Page 421.<br  />
 <em>Updating mean and variance estimates: an improved method</em>, D.H.D. West, 1979.<br  />
 Geisser and Cornfield, 1963, <em>Posterior distributions for multivariate normal parameters</em>.<br  />
</dd></dl>
<dl class="test"><dt><b><a class="el" href="test.html#_test000830">Test:</a></b></dt><dd><a class="el" href="namespacetest__pm__sampleCov.html">test_pm_sampleCov</a></dd></dl>
<dl class="bug"><dt><b><a class="el" href="bug.html#_bug000066">Bug:</a></b></dt><dd><br  />
<b>Status:</b> See <span style="color:red"><b>Unresolved</b></span>, See <a href="https://groups.google.com/g/comp.lang.fortran/c/NDE6JKTFbNU">this page</a> for more information.<br  />
 <br  />
<b>Source:</b> GNU Fortran Compiler <code>gfortran</code> <br  />
<b>Description:</b> Ideally, there should be only one generic interface in this module for computing the biased/corrected/weighted variance.<br  />
 This requires ability to resolve the different weight types, which requires custom derived types for weights.<br  />
 Fortran PDTs are ideal for such use cases. However, the implementation of PDTs is far from complete in GNU Fortran Compiler <code>gfortran</code>.<br  />
 <br  />
<b>Remedy</b> (as of ParaMonte Library version 2.0.0): Given that the importance of GNU Fortran Compiler <code>gfortran</code> support, separate generic interfaces were instead developed for different sample weight types.<br  />
 Once the GNU Fortran Compiler <code>gfortran</code> PDT bugs are resolved, the <a class="el" href="interfacepm__sampleVar_1_1getVar.html">getVar</a> generic interface can be extended to serve as a high-level wrapper for the weight-specific generic interfaces in this module.<br  />
</dd></dl>
<p ><br  />
<a class="anchor" id="finmain"></a><b>Final Remarks</b> <a href="#finmain">⛓</a> </p><hr  />
<p> If you believe this algorithm or its documentation can be improved, <b>we appreciate your contribution and help</b> <a href="https://github.com/cdslaborg/paramonte/tree/main/src/fortran/main/pm_sampleVar.F90#L379 " target="_blank"><b>to edit this page's documentation and source file on GitHub</b></a>.<br  />
 For details on the naming abbreviations, see <a class="el" href="index.html#ParaMonteLangAbbreviationGuidlines">this page</a>.<br  />
 For details on the naming conventions, see <a class="el" href="index.html#ParaMonteLangNamingConventions">this page</a>.<br  />
 This software is distributed under the <a href="https://github.com/cdslaborg/paramonte#license" target="_blank">MIT license</a> <b>with additional terms outlined below.</b><br  />
</p><ol>
<li>
If you use any parts or concepts from this library to any extent, <b>please acknowledge the usage by citing the relevant</b> <a href="https://www.cdslab.org/paramonte/generic/latest/overview/preface/#how-to-acknowledge-the-use-of-the-paramonte-library-in-your-work" target="_blank"><b>publications of the ParaMonte library</b></a>.<br  />
</li>
<li>
If you regenerate any parts/ideas from this library in a programming environment other than those currently supported by this ParaMonte library (i.e., other than C, C++, Fortran, MATLAB, Python, R), <b>please also ask the end users to</b> <a href="https://www.cdslab.org/paramonte/generic/latest/overview/preface/#how-to-acknowledge-the-use-of-the-paramonte-library-in-your-work" target="_blank"><b>cite this original ParaMonte library</b></a>.<br  />
</li>
</ol>
<p>This software is available to the public under a highly permissive license.<br  />
Help us justify its continued development and maintenance by acknowledging its benefit to society, distributing it, and contributing to it.<br  />
 </p><dl class="section copyright"><dt>Copyright</dt><dd><a href="https://www.cdslab.org" target="_blank">Computational Data Science Lab</a></dd></dl>
<dl class="todo"><dt><b><a class="el" href="todo.html#_todo000366">Todo:</a></b></dt><dd><span class="pmed">Normal Priority</span>: The inclusion of bias correction in the calculation of variance is a frequentist abomination and shenanigan that must be eliminated in the future.<br  />
 The correction factor should be computed separately from the actual variance calculation.<br  />
</dd></dl>
<dl class="authors"><dt><b><a class="el" href="authors.html#_authors001212">Author:</a></b></dt><dd><a href="https://www.github.com/shahmoradi" target="_blank">Amir Shahmoradi</a>, Nov 24, 2020, 4:19 AM, Dallas, TX<br  />
 <a href="https://www.faba.one" target="_blank">Fatemeh Bagheri</a>, Thursday 12:45 AM, August 20, 2021, Dallas, TX<br  />
 <a href="https://www.github.com/shahmoradi" target="_blank">Amir Shahmoradi</a>, Monday March 6, 2017, 2:48 AM, Institute for Computational Engineering and Sciences (ICES), The University of Texas at Austin.<br  />
 </dd></dl>
</div><h2 class="groupheader">Variable Documentation</h2>
<a id="ad9c411a07e915cb9810cf2091d79d67b" name="ad9c411a07e915cb9810cf2091d79d67b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad9c411a07e915cb9810cf2091d79d67b">&#9670;&nbsp;</a></span>MODULE_NAME</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">character(*, SK), parameter pm_sampleVar::MODULE_NAME = &quot;@pm_sampleVar&quot;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="pm__sampleVar_8F90_source.html#l00403">403</a> of file <a class="el" href="pm__sampleVar_8F90_source.html">pm_sampleVar.F90</a>.</p>

</div>
</div>
</div><!-- contents -->
</div><!-- doc-content -->
    <!-- start footer part -->
    <div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
      <ul>
        <li class="navelem"><a class="el" href="namespacepm__sampleVar.html">pm_sampleVar</a></li>
        <li class="footer">Generated on Sat May 11 2024 15:59:40 for ParaMonte Fortran 2.0.0 by <a href="http://www.doxygen.org/index.html" target="_blank"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.3 </li>
      </ul>
    </div>
    <div style="color: #ffffff; float: none; width: 100%;">
        <span style="font-size:0.8em">
            <script type="text/javascript">
                var sc_project=12178963; 
                var sc_invisible=1; 
                var sc_security="e0dfe0f9"; 
                var sc_text=3; 
                var scJsHost = "https://";
                document.write("<sc"+"ript type='text/javascript' src='" +
                scJsHost+
                "statcounter.com/counter/counter.js'></"+"script>");
            </script>
        </span>
    </div>
</body>
</html>
